<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="DHd2022_175">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title type="full">
                    <title type="main">Text-Bild-Gefüge</title>
                    <title type="sub">Digital Humanities und der Diskurs der Moderne</title>
                </title>
                <author>
                    <persName>
                        <surname>Klemstein</surname>
                        <forename>Franziska</forename>
                    </persName>
                    <affiliation>Bauhaus-Universität Weimar, Germany</affiliation>
                    <email>f.klemstein@gmail.com</email>
                <idno type="ORCID">0000-0003-3137-6732</idno></author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2015-10-05T00:04:51.74</date>
                </edition>
            </editionStmt>
            <publicationStmt>
    <publisher>Universität Potsdam</publisher>
    <address>
        <addrLine>Netzwerk für Digitale Geisteswissenschaften</addrLine>  
        <addrLine>Am Neuen Palais 10</addrLine>
        <addrLine>14469 Potsdam</addrLine>
        <addrLine>Deutschland</addrLine>
    </address>
    <publisher>Fachhochschule Potsdam</publisher>
    <address>
        <addrLine>Kiepenheuerallee 5</addrLine>
        <addrLine>14469 Potsdam</addrLine>
        <addrLine>Deutschland</addrLine>
    </address>
</publicationStmt>
            <sourceDesc>
                <p>Converted from an OASIS Open Document</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.22">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Posterpräsentation</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>Document Layout Analysis</term>
                    <term>Distant Reading</term>
                    <term>Media Theory</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>Modellierung</term>
                    <term>Annotieren</term>
                    <term>Kontextsetzung</term>
                    <term>Theoretisierung</term>
                    <term>Bilder</term>
                    <term>Text</term>
                </keywords>
            </textClass>
        <settingDesc><ab n="conference">DHd2022 – "Kulturen des digitalen Gedächtnisses", Potsdam</ab><ab n="paperID">175</ab></settingDesc></profileDesc>
    </teiHeader>
    <text>
        <body>
            <p>Die DH gelten als interdisziplinäres Feld par excellence. Sie bringen geisteswissenschaftliche Disziplinen, wie etwa die Linguistik oder die Geschichtswissenschaft, in enge Verbindung mit Disziplinen wie Computer Science und Information Science. Das daraus entstehende Potential zur übergreifenden Zusammenarbeit zwischen einzelnen geisteswissenschaftlichen Disziplinen wird bereits vielfach genutzt. Dennoch stehen textfokussierte und bildzentrierte Projekte oft unvermittelt nebeneinander. Gleichwohl haben multimodale Untersuchungsmethoden derzeit Konjunktur (Barman et al 2021).</p>
            <p>Zumeist findet dort jedoch eine Fokussierung auf sehr spezifische Einzelaspekte statt. So fokussierten Barman et al auf das Training weniger, exemplarisch ausgewählter Klassen, um ihre Netzwerke auf das Erkennen von Todesanzeigen innerhalb eines schweizerisch-luxemburgischen Korpus historischer Zeitungen zu trainieren. Bilder wurden hierbei als auffällige Layout-Bereiche (areas of interests) innerhalb eines strukturierten und sich wiederholenden Gesamtlayouts verstanden, nicht jedoch als eigenständige Bilder innerhalb eines Gesamtgefüges aus Text und Bild.</p>
            <p>Unser Forschungsprojekt nimmt dies zum Anlass um Texte und Bilder als gleichwertige Elemente sowohl im Bereich CV als auch im Bereich des NLP zu untersuchen. Innerhalb unseres ersten Projektabschnitts wird jedoch vor allem auf die Layout-Analyse und die Untersuchung des Verhältnisses von Text und Bild zueinander fokussiert. Konkret sollen dabei im ersten Teilprojekt u.a. folgende Fragen beantwortet werden:</p>
            <p>1.) Wie entwickelt sich in unserem Zeitschriftenkorpus die strukturelle Anordnung von Text und Bild?</p>
            <p>2.) Welche Rolle spielen dabei Bildunterschriften und wie wird im Text auf die Bilder Bezug genommen?</p>
            <p>3.) Welche Arten von Bildern und welche Bildinhalte werden in die Texte einbezogen und wann dominieren welche Bildarten in welchen Textgattungen bzw. Untersuchungsdomänen?</p>
            <p>Diese hier formulierten Fragen sollen eine systematische Untersuchung größerer Text-Bild-Korpora ermöglichen und damit das Text-Bild-Verhältnis innerhalb der sogenannten Moderne untersuchen. Unsere Hypothese lautet, dass Veröffentlichungen, wie zum Beispiel die Bauhausbücher als Ausdruck und Zeugnis des Diskurses der Moderne verstanden werden, da sie sich in ihrem Text-Bild-Verhältnis in einem Spektrum bewegen, das zwischen einer Verwissenschaftlichung künstlerischer Publikationen bzw. der Kunstpraxis einerseits steht und andererseits experimentelle Layoutversuche offenbart, die die Zweidimensionalität der Seite grundlegend in Frage stellt. Ob diese Hypothese zutrifft und tatsächlich als charakteristisch oder prägend für den Zeitraum der Moderne bezeichnet werden kann, muss durch eine möglichst breite und dennoch tiefgehende Analyse verifiziert werden. </p>
            <p>Um diese Hypothese nicht allein in Bezug auf die Bauhausbücher, sondern in einem größeren Kontext mit Blick auf die Veränderungen von Text-Bild-Gefügen innerhalb des Untersuchungszeitraums (1880–1930) untersuchen zu können, umfasst unser Korpus Publikationen (Zeitschriften, Monographien u.v.m.) aus verschiedenen Themenbereichen. Exemplarisch sind hier zu nennen: die Bauhausbücher, die Zeitschrift „Das Kunstgewerbe“, die „Fliegenden Blätter“, die „Zeitschrift für Psychologie und Physiologie der Sinnesorgane“ oder auch das „Centralblatt der Bauverwaltung“.</p>
            <p>Neben der Vorstellung der Untersuchungsdomänen und Textkorpora soll ebenso das methodische Vorgehen erläutert werden. So soll unter anderem der Annotationsprozess zur Erstellung des Datensatzes erläutert werden, auf dessen Grundlage dann zu einem späteren Zeitpunkt mithilfe des PubLayNet-Datensatzes (Zhong et al 2019) ein Mask R-CNN-Transferlernen (He et al 2017) für ein 19-Klassen-Problem (siehe Abb.1) durchgeführt werden soll. Da der Annotationsprozess selbst jedoch bereits zahlreiche nicht-triviale Entscheidungen in sich birgt, soll dieser hier besonders ausführlich beschrieben werden. Hierbei soll einerseits der Entscheidungsprozess für das Computer Vision Annotation Tool (CVAT) (Sekachev et al 2019) und gegen das Aletheia Document Analysis System (Clausner et al 2011) dargelegt werden sowie die Definition der zu annotierenden Klassen, die sich auch im Inter Annotator Agreement widerspiegeln und anderseits erste Analyseergebnisse vorgestellt werden, die auf der Grundlage der ersten Annotationen bereits möglich sind.</p>
            <p>Die in CVAT annotierten Digitalisate können in verschiedenen Formaten exportiert und auf diese Weise für verschiedene weitere Anwendungsmöglichkeiten nutzbar gemacht werden. Innerhalb unseres Projektes wird das Format „COCO 1.0“ genutzt, d.h. das nach dem Export ein Ordner mit den Digitalisaten sowie einer json-Datei vorliegen. Diese Datei bildet die Grundlage für erste oberflächliche Analysen, die jedoch bereits erste aufschlussreiche Anhaltspunkte für die Untersuchung verschiedener Textgattungen im Zeitraum von 1880 bis 1930 zulassen. Zu diesen gehören unter anderem folgende Abfragen:</p>
            <p>a) Welche Annotationsklassen sind auf welchen Seiten zu finden?</p>
            <p>b) Welche Seiten haben wie viele Textfelder bzw. Textspalten?</p>
            <p>c) Welche Seiten haben wie viele Abbildungen?</p>
            <p>d) Welche Seiten haben Bildunterschriften?</p>
            <p>Ebenso kann das Verhältnis von der spezifischen Klasse zur Gesamtseite oder im Verhältnis zu anderen Klassen ausgewertet werden und vieles mehr.</p>
            <p>Durch die Beantwortung dieser Fragen können nicht nur erste Annahmen zur strukturellen Anordnung von Text und Bild systematisch analysiert und hinterfragt werden, sondern zugleich eine Vergleichsebene zwischen verschiedenen Untersuchungsdomänen vorgenommen werden. Im Zentrum steht dabei die Frage, inwiefern sich die von Daston und Galison vorgenommene Ausweitung der Foucaultschen Diskursanalyse auf die (wissenschafts-)historische Untersuchung von Bilddiskursen (Daston/Galison 2007) durch den Gebrauch von verschiedenen digitalen Technologien konkretisieren und damit präzisieren lässt.</p>
            <p>
                <figure>
                    <graphic url="Pictures/9c867ccb994567faeaae75af1ab99e4a.jpg"/>
                    <head>Abb. 1.</head>
                </figure>
            </p>
        </body>
        <back>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliographie</head>
                    <bibl>
                        <hi rend="bold">Barman, R. / Ehrmann, M. / Clematide, S. / Oliveira, S. A. / Kaplan, F.</hi>: Combining Visual and Textual Features for Semantic Segmentation of Historical Newspapers, in: Journal of Data Mining &amp; Digital Humanities, DOI: 10.46298/jdmdh.6107 , arXiv:2002.06144.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Clausner, </hi>
                        <hi rend="bold">C. /</hi>
                        <hi rend="bold"> Pletschacher, </hi>
                        <hi rend="bold">S.</hi>
                        <hi rend="bold"> / Antonacopoulos, </hi>
                        <hi rend="bold">A.</hi>: Aletheia - An Advanced Document Layout and Text Ground-Truthing System for Production Environments, in: 
                        <emph>I</emph>
                        <emph>nternational Conference on Document Analysis and Recognition</emph>, 2011, pp. 48-52, doi: 10.1109/ICDAR.2011.19.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Daston, </hi>
                        <hi rend="bold">L. /</hi>
                        <hi rend="bold"> Galison, </hi>
                        <hi rend="bold">P.</hi>: Objektivität, Frankfurt/Main 2007.
                    </bibl>
                    <bibl>
                        <hi rend="bold">He, </hi>
                        <hi rend="bold">K. / </hi>
                        <hi rend="bold">Gkioxari, </hi>
                        <hi rend="bold">G. / </hi>
                        <hi rend="bold">Dollár, </hi>
                        <hi rend="bold">P. / </hi>
                        <hi rend="bold">Girshick, </hi>
                        <hi rend="bold">R.</hi>: Mask R-CNN, arXiv:1703.06870.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Sekachev, </hi>
                        <hi rend="bold">B. / </hi>
                        <hi rend="bold">Zhavoronkov, </hi>
                        <hi rend="bold">A. / </hi>
                        <hi rend="bold">Manovich, </hi>
                        <hi rend="bold">N.</hi>: Computer Vision Annotation Tool: A Universal Approach to Data Annotation, 2019, https://software.intel.com/content/www/us/en/develop/articles/computer-vision-annotation-tool-a-universal-approach-to-data-annotation.html [letzter Zugriff: 13.07.2021].
                    </bibl>
                    <bibl>
                        <hi rend="bold">Zhong, </hi>
                        <hi rend="bold">X. / </hi>
                        <hi rend="bold">Tang, </hi>
                        <hi rend="bold">J. / </hi>
                        <hi rend="bold">Yepes, </hi>
                        <hi rend="bold">A. J.</hi>: PubLayNet: largest dataset ever for document layout analysis, arXiv:1908.07836.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>
