<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="DHd2022_194">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Building and Improving an OCR Classifier for Republican Chinese Newspaper Text</title>
                <author>
                    <persName>
                        <surname>Arnold</surname>
                        <forename>Matthias</forename>
                    </persName>
                    <affiliation>Heidelberg Centre for Transcultural Studies, Universität Heidelberg, Germany</affiliation>
                    <email>arnold@uni-heidelberg.de</email>
                <idno type="ORCID">0000-0003-0876-6177</idno></author>
                <author>
                    <persName>
                        <surname>Henke</surname>
                        <forename>Konstantin</forename>
                    </persName>
                    <affiliation>Institut für Computerlinguistik, Universität Heidelberg, Germany</affiliation>
                    <email>konstantin.henke@protonmail.ch</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2022-02-14T06:38:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
    <publisher>Universität Potsdam</publisher>
    <address>
        <addrLine>Netzwerk für Digitale Geisteswissenschaften</addrLine>  
        <addrLine>Am Neuen Palais 10</addrLine>
        <addrLine>14469 Potsdam</addrLine>
        <addrLine>Deutschland</addrLine>
    </address>
    <publisher>Fachhochschule Potsdam</publisher>
    <address>
        <addrLine>Kiepenheuerallee 5</addrLine>
        <addrLine>14469 Potsdam</addrLine>
        <addrLine>Deutschland</addrLine>
    </address>
</publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.22">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Posterpräsentation</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>Chinese text OCR</term>
                    <term>character segmentation</term>
                    <term>synthetic training data</term>
                    <term>OCR error correction</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>Datenerkennung</term>
                    <term>Programmierung</term>
                    <term>Methoden</term>
                    <term>Projekte</term>
                    <term>Forschung</term>
                    <term>Text</term>
                </keywords>
            </textClass>
        <settingDesc><ab n="conference">DHd2022 – "Kulturen des digitalen Gedächtnisses", Potsdam</ab><ab n="paperID">194</ab></settingDesc></profileDesc>
    </teiHeader>
    <text>
        <body>
            <div>
                <p>For more than a decade, Republican magazines and newspapers have been collected by institutes and projects now joined in the Centre for Asian and Transcultural Studies (CATS) at Heidelberg University. Our platform “Early Chinese Periodicals Online” (ECPO, https://uni-heidelberg.de/ecpo), provides open access to more than 300.000 digital images and their metadata, cf. Arnold and Hessel (2020) . Since the material consists mostly of image scans, the project ran a number of experiments to explore possible approaches towards full text generation (Arnold, 2021). For newspapers printed in Latin scripts much has changed since Rose Holley commented item “Use the ‘training’ facility (artificial intelligence) in the OCR software” with “Not viable for cost effective mass scale digitization” and noted “Do not pursue” in her list of “Potential methods of improving OCR accuracy suggested by ANDP team” (Holley, 2009, table 2, item 9). Today, when researchers write that “transforming [historical newspapers] into machine-readable data by means of OCR poses some major challenges” they do that while they introduce their own OCR pipeline (Holley, 2009).</p>
                <p>Unfortunately, these approaches cannot just be adopted to historical Chinese newspapers. As we have shown, especially complex layout and resulting difficulties of reliable automatic page segmentation have so far prevented full text generation of these newspapers even within China (Arnold, 2021; Arnold, forthcoming; Arnold et al., forthcoming). In this long abstract we present the first results from a systematic approach towards full text extraction from a Republican China newspaper (
                    
                    1). Our basis is a small corpus for which text ground truth exists. We present our character segmentation method which produces about 90.000 images of characters. Based on the hypothesis that pre-training on extensive amounts of suitably augmented character images will increase the OCR accuracy for evaluation on real-life character image data, we generate synthetic training data. We then compare the OCR recognition results and show that a combination of synthetic and real characters produces best results. Finally, we propose a method that makes use of a masked language model for OCR error correction.</p>
                <figure>
                    <graphic n="1001" width="16.002cm" height="10.617222222222223cm" url="Pictures/88410662d449d33e65a5cff3b995e861.png" rend="inline"/>
                    <head>Fig. 1: An example fold from 
                        <hi rend="italic" xml:space="preserve">Jing bao </hi>
                        <hi rend="chinese">晶報</hi>
                        <hi rend="italic" xml:space="preserve"> (The Crystal)</hi>, April 21, 1939, pages 2-3.
                    </head>
                </figure>
                <p>Note: We will treat single rectangular text blocks (Fig. 2) as given and proceed from here to present effective methods for generating a data set later used to train an OCR model. We show that pre-training on artificially created training data can significantly improve OCR accuracy. Due to the limited scope of the presented experiments, this approach is still limited in terms of retrieved glyph size, image quality and font style, hence the model is not necessarily directly applicable to other historical Chinese documents.</p>
                <figure>
                    <graphic n="1002" width="16.002cm" height="7.1049444444444445cm" url="Pictures/81db20fc203518dfb858c7f6083e632d.png" rend="inline"/>
                    <head>Fig. 2: Manually cropped text blocks</head>
                </figure>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>The Corpus</head>
                <p>Our corpus consists of 9.385 scanned folds from the entertainment newspaper Jing bao 
                    <hi rend="chinese">晶報</hi> (The Crystal), published 03.03.1919–23.05.1940 (Fig. 1). The double-keyed text ground truth comprises all April 1939 issues (40 folds, ~245.000 characters). Aside from text blocks and their headings, it also contains mastheads, advertisements and marginalia, however, the methods presented below will solely focus on “header-less” text blocks of uniform font-size.
                </p>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>Character Segmentation</head>
                <p>Due to Chinese characters’ nearly squared appearance, it is common to find resulting text blocks implicitly displaying a grid layout (see Fig. 2). Deviation from the grid usually appears when additional characters had to be squeezed into one column or because of inaccurate printing. In order for the method described below to work, we manually sort out any text blocks that don’t adhere to the grid layout and then extract the corresponding ground truth section for every crop.</p>
                <p>After adaptive binarization (kernel size: 125 px) we calculate horizontal and vertical projection profiles, cf. Fan et al. (1998). To perform deskewing, we find an angle 
                    <hi rend="math">α</hi> with 
                    <hi rend="math">α</hi>
                    <hi rend="math">∈</hi> [-2.0,-1.5,...,2.0] such that rotating the image by <hi rend="math">α</hi> maximizes
                </p>
                <figure>
                    <graphic n="1003" width="5.023669444444445cm" height="0.4956111111111111cm" url="Pictures/8cd62edc9a090ec3418a00d22f89315d.png" rend="inline"/>
                </figure>
                <p>where w and h are the width and height of the image, c<hi rend="italic subscript">i</hi> is the number of black pixels in the <hi rend="italic">i</hi>-th column (= the corresponding value of the vertical projection profile) and l<hi rend="italic subscript">j</hi> in the <hi rend="italic">j</hi>-th line.
                </p>
                <p>After deskewing, we cut the gray-scale, non-binarized original text block image into single character images along separators defined by the following heuristic:</p>
                <p>(1) Use the valleys of the vertical projection profile to define separators between the columns.</p>
                <p>(2) Use the valleys of the horizontal (global) projection profile to define separators between the lines.</p>
                <p>(3) For every column, produce another (local) projection profile. If a local separator lies within 7px distance of a global separator defined by (2), discard the global separator and only use the local separator; else only use the global separator.</p>
                <p>The positions of the valleys are obtained by scipy.signal.find_peaks using a minimum distance of (1) 22, (2) 20 and (3) 14.</p>
                <p>For normalization and contrast enhancing the following method is used:</p>
                <list type="ordered">
                    <item>Globally (whole crop): Employ partial adaptive thresholding: Every pixel whose gray-scale value is larger (= brighter) than the average of a surrounding 7x7-kernel is set to 255 (white). Separately, every pixel whose value is greater than the median of the image (called threshold below) is assumed to be a background pixel and set to 255. Every other pixel keeps its gray-scale value. Choosing the median arises from the supposition that there are more background than content pixels.</item>
                    <item>Locally (after cropping rectangles containing one character each): Ignoring white pixels, linearly re-scale pixel values from [minval,threshold] to [0,255], where minval refers to the darkest pixel in the image. This allows even for very lightly printed characters to appear darker and have their decisive features more strongly separated from the background.</item>
                </list>
                <p>Finally, the resulting fields can be easily mapped to the ground truth text. Indentations have to be manually marked, and since the CNN (cf. Section 3.) requires squared images as input, we add white padding to transform the rectangular character images into square ones.</p>
                <p>This method entirely relies on correct annotation. While we can easily detect errors like missing lines, this is harder for missing or extra characters within a line (checking the line length), and basically impossible for typos or swapped characters. To avoid such mistakes we can only double-check annotations, otherwise they lower recognition accuracy.</p>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>Character Image Generation</head>
                <p>The method described in the section above yields a total of 92.039 character images (47.986 train + 21.676 dev + 22.377 test). Due to the Zipfian distribution, we additionally present the following table:</p>
                <table rend="rules">
                    <head>Tab. 1</head>
                    <row>
                        <cell style="text-align: left;">x</cell>
                        <cell style="text-align: left;">number of characters with at least x samples </cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">1</cell>
                        <cell style="text-align: left;">3045</cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">2</cell>
                        <cell style="text-align: left;">2355</cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">3</cell>
                        <cell style="text-align: left;">1995</cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">…</cell>
                        <cell style="text-align: left;">…</cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">10</cell>
                        <cell style="text-align: left;">1091</cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">…</cell>
                        <cell style="text-align: left;">…</cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">20</cell>
                        <cell style="text-align: left;">696</cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">…</cell>
                        <cell style="text-align: left;">…</cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">50</cell>
                        <cell style="text-align: left;">301</cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">…</cell>
                        <cell style="text-align: left;">…</cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">100</cell>
                        <cell style="text-align: left;">137</cell>
                    </row>
                </table>
                <p>Motivated by the low quantity of training samples for higher x, we generate additional synthetic training data and propose the following research hypothesis:</p>
                <p>Pre-training on extensive amounts of suitably augmented character images will increase the OCR accuracy for evaluation on real-life character image data.</p>
                <p>With the goal of imitating the real-life character images with artificial training data, we apply the following, partly randomized (in b., e2.2, f., g., and h.) augmentations to glyph images extracted from various fonts:</p>
                <figure>
                    <graphic n="1004" width="15.963194444444444cm" height="2.1060833333333333cm" url="Pictures/079830a1110024dfb8a6685e0c624492.png" rend="inline"/>
                    <head>Fig. 4: Augmentations to glyph images</head>
                </figure>
                <list type="ordered">
                    <item>Extract PNG images of a predefined set of glyphs from a Song-Ti font (= the font-style used in the newspapers).</item>
                    <item>Add random noise (peppering). </item>
                    <item>Use morphological opening and then closing to enlarge noise pixels, grow them together with other close-by black pixels (other noise or the actual character) during erosion (= dilation of black contours on white background) and remove useless noise during dilation (= erosion of black pixels). </item>
                    <item>Use erosion to thicken lines. </item>
                    <item>Emphasize vertical lines while blurring and staining the remaining parts: 
                        <list type="ordered">
                            <item>Extract vertical elements of a certain minimum length using dilation with a vertical kernel. </item>
                            <item>Separately apply the following: 
                                <list type="ordered">
                                    <item>Further erode and blur the image. </item>
                                    <item>Generate random patches. </item>
                                    <item>Add the patches to the image. </item>
                                </list>
                            </item>
                            <item>Join the result and the previously extracted vertical lines using bitwise AND. </item>
                        </list>
                    </item>
                    <item>Blur the image once more. Additionally, brightness can be randomly in-/decreased before. Afterwards, linearly rescale pixel values to cover the whole 0-255 range, like the real-life images. </item>
                    <item>Apply randomized elastic transformation. </item>
                    <item>Add padding and perform appropriate resizing.</item>
                </list>
                <p>Since ultimately, the classes used for OCR are Unicode points, the question arises which code points to synthesize additional training data from. We employ the simple heuristic of using all of the glyphs featured in the ground truth, and adding any missing ones from the 4000 most frequent characters of a representative corpus. Furthermore, inconsistencies caused by Han-unification have to be solved. For example, the image data features 
                    <hi rend="chinese">靑</hi> instead of 
                    <hi rend="chinese">青</hi> and 
                    <hi rend="chinese">淸</hi> instead of 
                    <hi rend="chinese">清</hi> (all different code points), however only one code point exists for every other character containing 
                    <hi rend="chinese">靑</hi>/
                    <hi rend="chinese">青</hi> as a component (
                    <hi rend="chinese">請</hi>, 
                    <hi rend="chinese">情</hi>, 
                    <hi rend="chinese">靜</hi>, …). While 
                    <hi rend="chinese">值</hi> and 
                    <hi rend="chinese">値</hi> (the latter being the variant used in our image data) have different code points, their right component itself (
                    <hi rend="chinese">直</hi>) is Han-unified, etc. We decide to always use the most accurate code point as long as it’s not part of the CJK Compatibility Ideographs block (U+F900...U+FAFF), so e.g. 
                    <hi rend="chinese">令</hi> (U+4EE4) is used instead of 
                    <hi rend="chinese">令</hi> (U+F9A8), even though the latter might appear more accurate, depending on the font. Generally, we find that the character variants printed in our image data to be visually closer to the Japanese standard (e.g. the components 
                    <hi rend="chinese">爫</hi> and 
                    <hi rend="chinese">⺬</hi>), so we choose several Japanese fonts for training data generation.
                </p>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>Character Recognition</head>
                <p>We decide on using a GoogleNet CNN architecture (Szegedy et al., 2015), slightly modified to take 1-channel inputs instead of RGB-images. This has proven to be effective regarding both printed and handwritten Chinese character recognition, e.g. Zhong et al. (2015) and Xu et al. (2018). Training on different character image sets, we obtain the following top-k accuracies on the real-life validation set for 
                    <hi rend="italic">k</hi>
                    <hi rend="math">∈</hi>{1,...,10}:
                </p>
                <table rend="rules">
                    <head>Tab. 2</head>
                    <row>
                        <cell style="text-align: right;">k</cell>
                        <cell style="text-align: right;">1</cell>
                        <cell style="text-align: right;">2</cell>
                        <cell style="text-align: right;">3</cell>
                        <cell style="text-align: right;">4</cell>
                        <cell style="text-align: right;">5</cell>
                        <cell style="text-align: right;">6</cell>
                        <cell style="text-align: right;">7</cell>
                        <cell style="text-align: right;">8</cell>
                        <cell style="text-align: right;">9</cell>
                        <cell style="text-align: right;">10</cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">only synthetic character images (4 different fonts)</cell>
                        <cell style="text-align: right;">69.73</cell>
                        <cell style="text-align: right;">78.3</cell>
                        <cell style="text-align: right;">81.68</cell>
                        <cell style="text-align: right;">83.65</cell>
                        <cell style="text-align: right;">84.99</cell>
                        <cell style="text-align: right;">86.06</cell>
                        <cell style="text-align: right;">86.87</cell>
                        <cell style="text-align: right;">87.49</cell>
                        <cell style="text-align: right;">87.97</cell>
                        <cell style="text-align: right;">88.46</cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">only real character images</cell>
                        <cell style="text-align: right;">96.47</cell>
                        <cell style="text-align: right;">97.29</cell>
                        <cell style="text-align: right;">97.46</cell>
                        <cell style="text-align: right;">97.56</cell>
                        <cell style="text-align: right;">97.61</cell>
                        <cell style="text-align: right;">97.66</cell>
                        <cell style="text-align: right;">97.68</cell>
                        <cell style="text-align: right;">97.69</cell>
                        <cell style="text-align: right;">97.69</cell>
                        <cell style="text-align: right;">97.71</cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">pretraining on synthetic; fine-tuning on real</cell>
                        <cell style="text-align: right;">97.63</cell>
                        <cell style="text-align: right;">98.57</cell>
                        <cell style="text-align: right;">98.78</cell>
                        <cell style="text-align: right;">98.91</cell>
                        <cell style="text-align: right;">98.98</cell>
                        <cell style="text-align: right;">99.01</cell>
                        <cell style="text-align: right;">99.07</cell>
                        <cell style="text-align: right;">99.1</cell>
                        <cell style="text-align: right;">99.12</cell>
                        <cell style="text-align: right;">99.13</cell>
                    </row>
                </table>
                <p>We also find that the selection of the fonts by which variants (i.e. mainland Chinese, Taiwanese, Japanese, Korean) it was designed for is largely negligible, i.e. a Taiwanese font may score higher than a Japanese font, even though the latter features glyph variants closer to those found in our data. This is probably because the percentage of characters with regional variants is relatively small, and also implies that the characters’ stroke length and distance as well as small variations in the size of single character components is more relevant to the OCR accuracy when evaluating on real-life character images.</p>
                <p>Interestingly, while there is a huge difference in performance after training on synthetic vs. real data, the human eye is barely able to differentiate between even a big selection of synthetic and real character images if presented next to each other (cf. Fig. 5).</p>
                <figure>
                    <graphic n="1005" width="10.87261111111111cm" height="11.812763888888888cm" url="Pictures/c9238e687e368a2d3ffdd6c96f4c82a3.png" rend="inline"/>
                    <head>Fig. 5: Comparison between synthetic (top) and real (bottom) character images</head>
                </figure>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>OCR Error Correction</head>
                <p>Finally, we aim to improve top-1 accuracy values by using language models to find the correct character among the second to k-th prediction. As can be seen in the table in Section 3, there is a significant jump from top-1 to top-2 accuracy, meaning that for wrong predictions the gold character is often predicted in the second position.</p>
                <p>Inspired by Wang et al. (2019), we propose a method that identifies characters likely to be incorrect: Let x1 and x2 denote the logit scores of the top 2 candidates output by the OCR model. Now we set a threshold t for the difference between x1 and x2. Any OCR prediction where x1 − x2 &lt; t is treated as likely to be incorrect and is passed on to the correction step. This step works by having a pre-trained BERT model re-predict the character from the top k OCR candidates. Systematically testing for different combinations of t and k (with t 
                    <hi rend="math">∈</hi> [0,0.5, . . . ,10] and k 
                    <hi rend="math">∈</hi> [0,1, . . . ,18]), we settle with t = 2.5 and k = 7, where we attain the following final results:
                </p>
                <table rend="rules">
                    <head>Tab. 3</head>
                    <row>
                        <cell style="text-align: left;"/>
                        <cell style="text-align: center;">Development set</cell>
                        <cell style="text-align: center;">Test set</cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">Only OCR w/o pre-training</cell>
                        <cell style="text-align: center;">96.54</cell>
                        <cell style="text-align: center;">95.49</cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">Only OCR w/ pre-training</cell>
                        <cell style="text-align: center;">97.63</cell>
                        <cell style="text-align: center;">96.95</cell>
                    </row>
                    <row>
                        <cell style="text-align: left;">OCR w/ pre-training + BERT-based correction</cell>
                        <cell style="text-align: center;">98.05</cell>
                        <cell style="text-align: center;">97.44</cell>
                    </row>
                </table>
                <p>As becomes evident, the presented post-processing method reduces the error by 18.1% (dev. set) / 16.1 % (test set).</p>
            </div>
        </body>
        <back>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliographie</head>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Arnold, Matthias</hi> (2021): 
                        <hi rend="italic">Ground Truth, Neural Networks, OCR: Towards Full Text of Republican China Newspapers</hi>. https://tinyurl.com/ecpo-intro [letzter Zugriff 15. Juli 2021].
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Arnold, Matthias</hi> (forthcoming): "Multilingual research projects: Challenges for making use of standards, authority files, and character recognition", in: 
                        <hi rend="italic">Digital Studies / Le champ numérique.</hi>
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Arnold, Matthias / Hessel, Lena</hi> (2020): "Transforming data silos into knowledge: Early Chinese Periodicals Online (ECPO)", in: Heuveline, Vincent / Gebhart, Fabian / Mohammadianbisheh, Nina (Hrsg.):
                        <hi rend="italic" xml:space="preserve"> E-Science-Tage 2019: Data to Knowledge</hi>. Heidelberg: heiBOOKS. S. 95–109. 10.11588/heibooks.598.c8420.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Arnold, Matthias / Paterson, Duncan / Xie, Jia</hi> (forthcoming): "Procedural Challenges: Machine Learning tasks for OCR of historical CJK newspapers", in: 
                        <hi rend="italic">International Journal of Digital Humanities.</hi>
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Fan, Kuo-Chin / Wang, Liang-Shen / Tu, Yin-Tien</hi> (1998): "Classification of Machine-Printed and Handwritten Texts Using Character Block Layout Variance", in: 
                        <hi rend="italic">Pattern Recognition</hi> 31, S. 1275–1284.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Holley, Rose</hi> (2009): "How Good Can It Get? Analysing and Improving OCR Accuracy in Large Scale Historic Newspaper Digitisation Programs", in: 
                        <hi rend="italic">D-Lib Magazine</hi> 10.1045/march2009-holley.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Liebl, Bernhard / Burghardt, Manuel</hi> (2020): "From Historical Newspapers to Machine-Readable Data: The Origami OCR Pipeline", in: 
                        <hi rend="italic">Proceedings of the Workshop on Computational Humanities Research (CHR 2020).</hi> Amsterdam, the Netherlands. S. 351–373. (= CEUR Workshop Proceedings). http://ceur-ws.org/Vol-2723/long20.pdf [letzter Zugriff 15. Juli 2021].
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Sung, Doris / Sun, Liying / Arnold, Matthias</hi> (2014): "The Birth of a Database of Historical Periodicals: Chinese Women’s Magazines in the Late Qing and Early Republican Period", in: 
                        <hi rend="italic" xml:space="preserve">Tulsa Studies in Women’s Literature </hi>33, S. 227–237.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Szegedy, Christian et al</hi>. (2015): "Going deeper with convolutions", in: 
                        <hi rend="italic">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</hi> Boston, MA. S. 1–9. 10.1109/CVPR.2015.7298594.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Wang, Hsiang-An / Liu, Pin-Ting</hi> (2019): "Towards a Higher Accuracy of Optical Character Recognition of Chinese Rare Books in Making Use of Text Model", in: 
                        <hi rend="italic">Proceedings of the 3rd International Conference on Digital Access to Textual Cultural Heritage.</hi> New York, NY, USA: Association for Computing Machinery. S. 15–18. (= DATeCH2019). 10.1145/3322905.3322922.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Xu, Xin et al.</hi> (2018): "Chinese Characters Recognition from Screen-Rendered Images Using Inception Deep Learning Architecture", in: Zeng, Bing et al. (Hg.): 
                        <hi rend="italic">Advances in Multimedia Information Processing – PCM 2017.</hi> Cham: Springer International Publishing. S. 722–732. (= Lecture Notes in Computer Science).
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Zhong, Zhuoyao / Jin, Lianwen / Xie, Zecheng</hi> (2015): "High performance offline handwritten Chinese character recognition using GoogLeNet and directional feature maps", in: 
                        <hi rend="italic">2015 13th International Conference on Document Analysis and Recognition (ICDAR).</hi> Tunis, Tunisia. S. 846–850. 10.1109/ICDAR.2015.7333881.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>
