<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="DHd2022_240">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title type="full">
                <title type="main">AdA Annotation Explorer</title>
                <title type="sub">Ein Framework für zeitbasierte Linked Open Data-Annotationen zur Analyse audiovisueller Korpora</title>
                </title>
                <author>
                    <persName>
                        <surname>Agt-Rickauer</surname>
                        <forename>Henning</forename>
                    </persName>
                    <affiliation>Hasso-Plattner-Insitut, Universität Potsdam, Germany</affiliation>
                    <email>henning.agt@gmail.com</email>
                </author>
                <author>
                    <persName>
                        <surname>Scherer</surname>
                        <forename>Thomas</forename>
                    </persName>
                    <affiliation>Freie Universität Berlin, Germany</affiliation>
                    <email>scherer.thomas@fu-berlin.de</email>
                <idno type="ORCID">0000-0002-6746-2803</idno></author>
                <author>
                    <persName>
                        <surname>Stratil</surname>
                        <forename>Jasper</forename>
                    </persName>
                    <affiliation>Freie Universität Berlin, Germany</affiliation>
                    <email>jasper.stratil@fu-berlin.de</email>
                <idno type="ORCID">0000-0001-9091-8410</idno></author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2021-11-26T14:40:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
    <publisher>Universität Potsdam</publisher>
    <address>
        <addrLine>Netzwerk für Digitale Geisteswissenschaften</addrLine>  
        <addrLine>Am Neuen Palais 10</addrLine>
        <addrLine>14469 Potsdam</addrLine>
        <addrLine>Deutschland</addrLine>
    </address>
    <publisher>Fachhochschule Potsdam</publisher>
    <address>
        <addrLine>Kiepenheuerallee 5</addrLine>
        <addrLine>14469 Potsdam</addrLine>
        <addrLine>Deutschland</addrLine>
    </address>
</publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.22">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Vortrag</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>Linked Open Data</term>
                    <term>Filmanalyse</term>
                    <term>Videoannotation</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>Entdeckung</term>
                    <term>Programmierung</term>
                    <term>Annotieren</term>
                    <term>Stilistische Analyse</term>
                    <term>Visualisierung</term>
                    <term>Video</term>
                </keywords>
            </textClass>
        <settingDesc><ab n="conference">DHd2022 – "Kulturen des digitalen Gedächtnisses", Potsdam</ab><ab n="paperID">240</ab></settingDesc></profileDesc>
    </teiHeader>
    <text>
        <body>
            <p style="text-align: left; ">Die Medienspezifik zeitbasierter performativer Künste, wie Musik, Theater, Tanz und auch Film, stellt Analyseansätze, die nach der ästhetischen Erfahrung fragen, vor zahlreiche Herausforderungen: Es geht in diesen Ansätzen nicht um die ‘objektiv-messbaren’ Eigenschaften der Artefakte und Aufführungen, sondern darum, die intersubjektive Erfahrungsdimension in den Blick zu nehmen – sie zielen auf das Denken und Fühlen, das im Filme-Sehen (Kappelhoff 2018) oder Musik-Hören entsteht. Ein tradiertes analytisches Vorgehen besteht in der Dekonstruktion solcher Erfahrungsprozesse in detaillierte Beschreibungen zeitlicher Prozesse – von Partituren über Tanznotationen bis hin zu mehrdimensionalen Einstellungsprotokollen. Solche Instrumente und Techniken rücken vermehrt in den Fokus der Digital-Humanities-Forschung, wobei die Verbindung ästhetischer Theorien und Techniken der Datengewinnung, -haltung, -verarbeitung und -aufbereitung eine der zentralen Herausforderungen des Feldes markiert (Arnold et al. 2019, Flückiger/Halter 2020, Freedman et al. 2019). Drei Schlüsselaspekte eines solchen Methodendesigns werden im Folgenden anhand der Begriffe der 
                <hi rend="italic">Synchronizität</hi>, 
                <hi rend="italic">Prozessualität</hi> und 
                <hi rend="italic">Komparabilität</hi> beleuchtet. Audiovisuelle Bilder werden erst dann analytisch handhabbar, wenn sie auf unterschiedlichen Beschreibungsebenen gleichzeitig erfasst werden, um dann deren Zusammenspiel in den Blick zu nehmen (<hi rend="italic">Synchronizität</hi>). Dabei sind einzelne Beschreibungen immer kontextabhängig und nicht objektorientiert, sondern auf die zeitliche Entfaltung eines Ganzen bezogen (<hi rend="italic">Prozessualität</hi>). Das Forschungsinteresse ist dabei selten auf ein einzelnes Werk begrenzt, sondern richtet sich an ein Genre, einen Themenkomplex oder einen audiovisuellen Diskurs (<hi rend="italic">Komparabilität</hi>). Es besteht so die Anforderung nach komplexen, vielschichtigen Beschreibungen, die gleichzeitig jedoch auf einen größeren Korpus bezogen werden können. Dies übersteigt häufig die Kapazitäten einzelner Forscher*innen, jedoch ermöglichen systematisierte filmanalytische Annotationen gleichermaßen die Vergleichbarkeit der analytischen Beobachtungen mehrerer Annotator*innen, als auch die Integration (semi-)automatisch erzeugter Annotationen.
            </p>
            <p style="text-align: left; ">Einer solchen Korpusstudie widmete sich die Nachwuchsgruppe “Affektrhetoriken des Audiovisuellen” (2016–2021, Freie Universität Berlin/ Hasso-Plattner-Institut, kurz: “AdA”), die den audiovisuellen Diskurs zur globalen Finanzkrise (nach 2007) hinsichtlich wiederkehrender affektrhetorischer Muster untersuchte. Das Projekt gründete auf theoretischen Annahmen zum Filme-Sehen, dem Verhältnis der Wahrnehmung von Zuschauenden und sich zeitlich entfaltenden audiovisuellen Bilder und knüpfte an die eMAEX-Methode an (Kappelhoff et al. 2011-2016), die auf die analytischen Grundprinzipien von Segmentierung und Qualifizierung zurückgreift, um Erfahrungsfigurationen zu rekonstruieren (Bakels et al. 2020a, 2020b). </p>
            <p style="text-align: left; ">Um eine feingliedrige und multidimensionale Beschreibung mit interpersoneller Annotation für einen größeren Korpus leisten zu können, galt es das filmanalytische Beschreibungsvokabular in der AdA-Filmontologie maschinenlesbar zu systematisieren (für eine ausführlichere Darstellung siehe Bakels et al. 2020c). Das resultierende systematische Vokabular 
                <hi rend="background(white)">orientiert sich ebenso an methodischen Standardbegriffen der Filmwissenschaft (z.B. Einstellungsgrößen oder Montagefiguren), wie an ansatzspezifischen Fachbegriffen der eMAEX-Analyse (z.B. ‘Ausdrucksbewegung’ oder ‘Dynamiken des Bildraums’). Die AdA-Filmontologie</hi> umfasst 502 einzelne Annotationswerte, die 78 Annotationstypen zugeordnet sind, die wiederum auf 8 Beschreibungsebenen wie Akustik, Montage, Bildkomposition oder Kamera organisiert sind (siehe Abb. 2).
            </p>
            <p style="text-align: left; ">Mit einem auf Basis der AdA-Filmontologie generierten Template konnten in der Annotationssoftware 
                <hi rend="italic">Advene</hi> (Aubert/Prié 2005) semantisch strukturierte Linked Open Data-Annotationen erstellt werden. Insgesamt wurden mehr als 125.000 Annotationen manuell erstellt für 4 komplette Filme, 2 Webvideos, Beiträge aus 35 Nachrichtensendungen sowie einzelne Szenen aus 10 weiteren Filmen. Darüber hinaus wurden über 440.000 Annotationen für 76 Filme sowie 315 Nachrichtensendungen automatisch generiert. Um die affektrhetorischen Muster auf Grundlage dieser manuellen und automatischen Annotationen allerdings korpusübergreifend analysieren zu können, insbesondere hinsichlich ihrem inhärenten multidimensionalen Zusammenspiel, d.h. auf unterschiedlichen Beschreibungsebenen, und kontextabhängig im zeitlichen Verlauf, war ein Framework zur Exploration des umfassenden Annotationsdatensatzes notwendig, für das verfügbare Annotations- und Visualisierungssoftware (wie Advene, ELAN, Dive+ der Clariah Media Suite) nicht ausgelegt war. Die zeitgleich entwickelte VIAN WebApp (<ref target="https://www.vian.app/">https://www.vian.app/</ref>) legt im Zusammenspiel mit der VIAN-Annotationssoftware wiederum den zentralen Untersuchungsfokus auf die Farbanalyse.
            </p>
            <div type="div1" rend="DH-Heading1">
                <head>
                    <anchor xml:id="lrr5kuewkcvp"/>Entwicklung &amp; Architektur
                </head>
                <p style="text-align: left; ">Der AdA Annotation Explorer<ref target="ftn1" n="1"/> wurde entwickelt, um sämtliche im AdA-Projekt erstellten zeitbasierten Videoannotationen zu verwalten und über eine webbasierte Oberfläche für die Exploration und Analyse zugänglich zu machen. Hierbei stand einerseits im Fokus, alle im Projekt entstehenden Daten zur freien Nutzung und Weiterverwendung zu veröffentlichen und andererseits die Anforderungen aus der korpusübergreifenden Filmanalyse umzusetzen, um Annotationsdaten gezielt nach bestimmten Kriterien abzufragen, zu filtern und die Ergebnisse kontextabhängig (d.h. in ihrer Einbettung in szenische Kompositionen) zu visualisieren.
                </p>
                <p style="text-align: left; ">Der Annotation Explorer wurde über eine entkoppelte Client-Server-Architektur mit RESTful API (Masse 2011) realisiert (siehe Abb. 1), die auf offene Standards für den Datenaustausch und auf Verwendung von Open-Source-Komponenten setzt.</p>
                <figure>
                    <graphic n="1001" width="15.92cm" height="5.503333333333333cm" url="Pictures/ece5ea2933a420fe8860392cd797e4aa.png" rend="inline"/>
                    <head>Abb. 1: Architektur des Annotation Explorers</head>
                </figure>
                <p style="text-align: left; ">Die Serverkomponenten im Backend sind für die Speicherung der Daten in einer Graphdatenbank (Triplestore) und für die Bereitstellung von Funktionen zum Datenabruf (REST-API) zuständig. Die Clientkomponenten des Frontends laufen ausschließlich im Browser und bilden die Benutzerschnittstelle, die eine Zusammenstellung von Annotationen mittels Eingabe von Auswahlkriterien (Filme, Szenen, Annotationswerte, etc.) ermöglicht und die Filterung und Weiterverarbeitung der Ergebnisse der REST-API übernimmt. Die aufbereiteten Daten werden mit der Open-Source-Software FrameTrail<ref target="ftn2" n="2"/> visualisiert, die es erlaubt, interaktive Videos und damit verlinkte Inhalte direkt im Browser anzusehen.
                </p>
                <div type="div2" rend="DH-Heading2">
                    <head>
                        <anchor xml:id="wyyms4lyy0cg"/>Datenhaltung
                    </head>
                    <p style="text-align: left; ">Das Datenmanagement des Annotation Explorers folgt den Linked Open Data-Prinzipien<ref target="ftn3" n="3"/>, um die in aufwändiger Arbeit erstellten Annotationen und Metadaten als maschinenlesbare, offene Daten für die Nachnutzung zu veröffentlichen. Sie werden mithilfe von Openlink Virtuoso<ref target="ftn4" n="4"/> gespeichert und mit LodView<ref target="ftn5" n="5"/> angezeigt. Der öffentliche SPARQL-Endpunkt<ref target="ftn6" n="6"/> erlaubt auch das Abfragen und Nutzen der Daten mit selbst entwickelten Queries. Die Nutzung von W3C Standards, die Bereitstellung von öffentlichen URIs für Annotationsdaten, Metadaten und Vokabular, sowie die dokumentierte Ontologie, ermöglichen die Nutzung der Projektergebnisse im Sinne der FAIR Data Prinzipien.
                    </p>
                    <p style="text-align: left; ">Zu den Daten des Projekts gehört die 
                        <hi rend="italic">AdA-Filmontologie</hi><ref target="ftn7" n="7"/>, ein systematisches Vokabular auf Basis von OWL und RDF, welches filmanalytische Begriffe – geordnet nach Annotationsebenen, Annotationstypen und Annotationswerten (siehe Abb. 2) – für feingranulare semantische Videoannotationen definiert und per URIs erreichbar macht<ref target="ftn8" n="8"/>.
                    </p>
                    <figure>
                        <graphic n="1002" width="16.002cm" height="10.059458333333334cm" url="Pictures/66f715fd74b02c02c414872dccd14d6b.png" rend="inline"/>
                        <head>Abb. 2: Klassenstruktur der AdA-Filmontologie am Beispiel kameraspezifischer Konzepte</head>
                    </figure>
                    <p style="text-align: left; ">Die 
                        <hi rend="italic">Korpus-Metadaten</hi> umfassen beschreibende Information (wie z.B. Titel, Laufzeit, Veröffentlichungsjahr, Regisseur) zu allen Filmen im Videokorpus zur globalen Finanzkrise. Die Metadatenfelder werden mit bestehenden Vokabular-Properties aus DBpedia, schema.org und Dublin Core kodiert und sind ebenfalls per URI abrufbar<ref target="ftn9" n="9"/>. Der Korpus umfasst 391 Filme in den Kategorien Spielfilme, Dokumentarfilme und Nachrichten<ref target="ftn10" n="10"/>.
                    </p>
                    <p style="text-align: left; ">
                        <hi rend="italic">Semantische Videoannotationen</hi> sind der Hauptteil der Daten, die im Triplestore gespeichert werden. Sie erfassen die zeitbezogenen Anmerkungen zu den Filmen auf den unterschiedlichen Beschreibungsebenen unter Verwendung des systematischen Vokabulars. Das Annotationsmodell basiert auf dem W3C Web Annotation Data Model<ref target="ftn11" n="11"/> und der Media Fragments URI Spezifikation<ref target="ftn12" n="12"/> (siehe Abb. 3).
                    </p>
                    <figure>
                        <graphic n="1003" width="16.002cm" height="5.642680555555556cm" url="Pictures/19cc39ceed971a2cf0af9d7614e2a795.png" rend="inline"/>
                        <head>Abb. 3: Beispielannotation einer Kamerafahrt für den Film “Company Men”<ref target="ftn13" n="13"/> als RDF-Graph visualisiert.</head>
                    </figure>
                </div>
                <div type="div2" rend="DH-Heading2">
                    <head>
                        <anchor xml:id="e42gvccq7crt"/>Datenabruf
                    </head>
                    <p style="text-align: left; ">Die im Projekt entwickelte REST-API bildet die Schnittstelle zwischen dem User Interface und den im Triplestore gespeicherten Videoannotationen, Metadaten und Ontologiedaten. Sie bietet über einen Web-Dienst per HTTP eine Reihe von Funktionen an, die entsprechend der Anfrageparameter SPARQL-Datenbankabfragen generieren, ausführen und direkt im Browser weiterverarbeitbare JSON bzw. JSON-LD Formate zurückliefern. Sie ist in Java implementiert und verwendet die Frameworks Apache Jena<ref target="ftn14" n="14"/>, Javalin<ref target="ftn15" n="15"/> und JSONLD-JAVA<ref target="ftn16" n="16"/>. Neben der Bereitstellung der Ontologiedaten und der Verwaltung der Metadaten, ist die Hauptaufgabe der REST-API, die feingliedrige und multidimensionale Beschreibung der Filme korpusübergreifend anhand bestimmter Kriterien abrufbar und durchsuchbar zu machen.
                    </p>
                    <p style="text-align: left; ">Videoannotationen können pro Film, pro Szene und pro Annotationstyp abgerufen werden (siehe Abb. 4). Die REST-API ermöglicht außerdem eine Volltextsuche nach einem oder mehreren Schlüsselwörtern und liefert alle Trefferannotationen korpusübergreifend zurück (Beispiel: “crisis”, nur ganze Worte, nur im Annotationstyp ‘Dialogtext’)<ref target="ftn17" n="17"/>. Um Muster im Korpus zu finden, ist eine Value Search nach ein oder mehreren Annotationswerten möglich. Dabei werden die Stellen zurückgeliefert, bei denen Werte 
                        <hi rend="italic">zeitgleich</hi> (auch teilweise überlappend) auftreten (z.B. Musikstimmung “traurig” zusammen mit nahen Einstellungsgrößen). 
                    </p>
                    <figure>
                        <graphic n="1004" width="16.002cm" height="8.080375cm" url="Pictures/756817275ce20c6019655add3953bccc.png" rend="inline"/>
                        <head>Abb. 4: Veranschaulichung des Datenabrufs für drei Annotationstypen aus zwei Szenen eines Films</head>
                    </figure>
                </div>
                <div type="div2" rend="DH-Heading2">
                    <head>
                        <anchor xml:id="pebkd0vfiauu"/>Frontend (Zusammenstellung &amp; Visualisierung)
                    </head>
                    <p style="text-align: left; ">Die 
                        <hi rend="italic">Zusammenstellungskomponente</hi> im Annotation Explorer Web-Frontend erlaubt der Benutzer*in Anfragen an die Datenbasis zu stellen. Dafür werden Formularfelder zur Film-, Text-, Bilder- und Wertesuche (siehe Abb. 5) mit nutzerfreundlicher Autocomplete-Funktion angeboten, um ein schnelles Auswählen aus der großen Menge von Metadaten zu ermöglichen. Entsprechend der Nutzereingaben werden REST-API Aufrufe erzeugt, Annotationen geladen und für den Arbeitsbereich zusammengestellt.
                    </p>
                    <figure>
                        <graphic n="1005" width="15.92cm" height="3.2808333333333333cm" url="Pictures/db767753d1efc70313dee5ab685a2708.png" rend="inline"/>
                        <head>Abb. 5: Übersicht der vier Anfragemöglichkeiten im Annotation Explorer</head>
                    </figure>
                    <p style="text-align: left; ">Filmmetadaten und Ontologiedaten werden im Filterbereich in zwei Baumansichten (Facettensuche) dargestellt, geben Aufschluss darüber, wie viele Annotationen in den jeweiligen Kategorien gefunden wurden, und ermöglichen das weitere Einschränken des Suchergebnisses.</p>
                    <p style="text-align: left; ">Die kontextabhängige 
                        <hi rend="italic" xml:space="preserve">Visualisierung </hi>der Annotationen erfolgt in einer Timeline-Darstellung zusammen mit einem Videoplayer, bei der angefragte Annotationstypen als Spuren dargestellt werden (siehe Abb. 6). Für diesen Zweck wird FrameTrail eingesetzt, das im Rahmen des Projekts durch den Autor Joscha Jäger hinsichtlich der Darstellung spezieller Annotationstypen, Sortiermöglichkeiten, Szenen- bzw. Filmvergleich und Exportmöglichkeiten erweitert wurde.
                    </p>
                </div>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>
                    <anchor xml:id="vctv87q7j23"/>Anwendung &amp; Analyse
                </head>
                <p style="text-align: left; ">Das graphische User-Interface des Annotation Explorers – der sogenannte Workspace – ist zentral für die Nutzer*inneninteraktion, das meint in erster Linie Film- und Medienwissenschaftler*innen ohne Programmierkenntnisse (siehe Abb. 6). Dabei kommen die drei zu Beginn genannten Schlüsselaspekte des Methodendesigns in der konkreten Nutzung des Annotation Explorers zum Tragen.</p>
                <figure>
                    <graphic n="1006" width="15.92cm" height="8.078611111111112cm" url="Pictures/07717f7c34cffc743b592e036e883e95.png" rend="inline"/>
                    <head>Abb. 6: Workspace des Annotation Explorers</head>
                </figure>
                <div type="div2" rend="DH-Heading2">
                    <head>
                        <anchor xml:id="lijrelrkob1k"/>Prozessualität
                    </head>
                    <p style="text-align: left; ">Gegenstand der eMAEX-basierten Filmanalyse ist eine sich in der Wahrnehmung entfaltende Bewegung. In diesem Kontext ist nicht nur die enge Verknüpfung der Annotationen mit dem interaktiven Videoplayer herauszustellen, der die direkte Rückbindung der Analysebeobachtungen an das Bewegtbild ermöglicht und dabei deren Einbettung erlebbar macht, vielmehr sind die Einzelannotationen nicht als statische Eigenschaften oder isolierte Zeitpunkte von Interesse, sondern stets in der zeitlichen Entfaltung und in Bezug auf größere Zeitsegmente (etwa szenische Kompositionen). </p>
                    <p style="text-align: left; ">Entscheidend ist dabei auch die Skalierbarkeit der Ansicht: so lassen sich sowohl einzelne Szenen als auch ganze Filme durch eine Szenenauswahlleiste oder freies Zoomen in den Blick nehmen.</p>
                   <figure>
                            <graphic n="1007" width="15.92cm" height="2.998611111111111cm" url="Pictures/fd53d0fa0ac6cd2bc78d1f37c5d5ed63.png" rend="inline"/>
                        </figure>
                        <figure>
                            <graphic n="1008" width="15.92cm" height="2.892777777777778cm" url="Pictures/88d93bcf9ac0300b40adfed5b79cc15f.png" rend="inline"/>
                            <head>Abb. 7: Optischer Fluss-Makroprofil eines Films (oben) und Detailansicht einer Szene (unten)</head>
                    </figure>
                    <p style="text-align: left; ">Mit dem Wechsel zwischen Makro und Mikro in der Ansicht ganzer Filme können Schlüsselsequenzen (z.B. dramaturgische Umschlagpunkte) anhand Extrema einzelner Analysedimensionen (etwa extreme Nahaufnahmen, besonders (un-)bewegte Abschnitte, sehr leise Passagen oder schnelle Wortwechsel auf Ebene des Dialogs) identifiziert werden.</p>
                    <p style="text-align: left; ">Der Annotation Explorer ist so gestaltet, dass nicht der einzelne analytische Befund in Annotationsform grafisch isoliert dargestellt wird, sondern stets als Teil eines komplexen Gefüges. So lassen sich durch unterschiedliche Darstellungsoptionen beispielsweise Einstellungsgrößen als farbiges Balkendiagramm anzeigen, bei der die Höhe des Balkens Auskunft über die Nähe der Kamera zum Referenzobjekt und die Breite Auskunft über die Dauer der einzelnen Einstellungen gibt (siehe Abb. 8). Auf diese Weise wird der Blick der Analysierenden auf den Prozess der Entfaltung im Hinblick auf spezifische Gestaltungsdimensionen gelenkt: Im Fall von Einstellungsgrößen die Modulation von Nähe-Distanz-Verhältnissen.</p>
                    <figure>
                        <graphic n="1009" width="15.92cm" height="1.7638888888888888cm" url="Pictures/22362ce90163b2467f60680229ae2b80.png" rend="inline"/>
                        <head>Abb. 8: Darstellung einer Abfolge von Einstellungsgrößen</head>
                    </figure>
                </div>
                <div type="div2" rend="DH-Heading2">
                    <head>
                        <anchor xml:id="e1cgb976tzjf"/>Synchronizität
                    </head>
                    <p style="text-align: left; ">Die Expressivität audiovisueller Bilder ist nicht auf eine Gestaltungsdimension zu reduzieren, sondern erst der Blick auf das Zusammenspiel unterschiedlicher Dimensionen gibt Aufschluss über die Affektdramaturgie einer Sequenz. So lassen sich im Annotation Explorer unterschiedliche Kombinationen von Annotationstypen zusammenstellen und über die Baumansichten (Facettensuchen) detailliert anpassen (siehe Abb. 9).</p>
                    <figure>
                        <graphic n="10010" width="15.92cm" height="10.371666666666666cm" url="Pictures/708c6a02a857adf90595d0ef45197b86.png" rend="inline"/>
                        <head>Abb. 9: Annotations-”Partitur” im Annotation Explorer.</head>
                    </figure>
                    <p style="text-align: left; ">Es lässt sich beispielsweise untersuchen wie ein akustisches Phänomen (z.B. Einsatz und Taktung von Musik) mit einem optischen Phänomen (z.B. Schnittrhythmus) interagiert, dieses aufgreift, verstärkt oder konterkariert. Die FrameTrail-Implementation im Workspace bietet die Möglichkeit unterschiedliche Visualisierungsformen auf den so abgerufenen Annotationsdatensatz anzuwenden und mit wenigen Klicks unterschiedliche Annotations-”Partituren” zu erstellen. Diese können entweder über die URL geteilt und gespeichert, oder als offline verfügbare HTML-Pakete heruntergeladen werden.</p>
                </div>
                <div type="div2" rend="DH-Heading2">
                    <head>
                        <anchor xml:id="sy5tjw1wvsj1"/>Komparabilität
                    </head>
                    <p style="text-align: left; ">Die Untersuchungsperspektive des AdA-Projekts und die Frage nach affektrhetorischen Mustern zielte aber nicht nur auf einzelne Filme und Sequenzen, sondern korpusübergreifende vergleichende Analysen. Dazu galt es die Möglichkeit bereitzustellen, Analyseabfragen verschiedener Filme direkt nebeneinander zu legen ohne an Funktionalität (interaktiver Videoplayer, Filter- und Suchoptionen, Darstellungsoptionen) einzubüßen. So lassen sich in der Playeransicht zwei Filme bzw. Szenen nebeneinander anzeigen, während weitere Vergleichsfilme/-szenen durch seitliches Scrollen ins Bild gebracht werden können (siehe Abb. 10).</p>
                    <figure>
                        <graphic n="10011" width="15.92cm" height="8.113888888888889cm" url="Pictures/9986288d8a2e3df83ae9d84c5d9c86a5.png" rend="inline"/>
                        <head>Abb. 10: Vergleich zweier Filme.</head>
                    </figure>
                    <p style="text-align: left; "></p>
                    <p style="text-align: left; ">In einer Listenansicht können wiederum mehrere Filme und Szenen untereinander angezeigt werden (siehe Abb. 11).</p>
                    <figure>
                        <graphic n="10012" width="15.92cm" height="10.830277777777777cm" url="Pictures/a3bf414328369f0fcb5843d15ecccc33.png" rend="inline"/>
                        <head>Abb. 11: Vergleich in der Listenansicht.</head>
                    </figure>
                    <p style="text-align: left; ">Zur Entwicklung und Überprüfung von Analysehypothesen gibt es unterschiedliche Suchfunktionen: etwa die Value Search oder die Volltextsuche, die für die Suche nach Schlagworten oder erwähnten Personen im Dialog (sei es auf Basis manueller Transkripte, redigierter Untertitel oder automatischer Spracherkennungsalgorithmen) genutzt werden kann oder auch für die Ergebnisse von automatischen Image Captioning-Extraktoren, das Aufspüren wiederkehrender oder ungewöhnlicher Bildelemente.</p>
                    <p style="text-align: left; ">Eine weitere Möglichkeit ist die Reverse Image Search, bei der eine Bilddatei als Suchanfrage hochgeladen werden kann. Diese wird dann mit Keyframes jeder Einstellung des gesamten Korpus abgeglichen und die ähnlichsten Bilder werden mit Metadaten versehen ausgegeben (siehe Abb. 12).</p>
                    <figure>
                        <graphic n="10013" width="9.672269444444444cm" height="6.145388888888889cm" url="Pictures/c6ddc31aa9b0531bd169ec436b884fd1.png" rend="inline"/>
                        <head>Abb. 12: Reverse Image Search im Annotation Explorer.</head>
                    </figure>
                    <p style="text-align: left; ">Die jeweiligen Anfrageergebnisse lassen sich wiederum im Framework des Annotation Explorers mit weiteren Annotationen kombinieren und können somit innerhalb sich zeitlich entfaltender Dynamiken, die auf dem Zusammenspiel unterschiedlicher Dimensionen beruhen, analysiert werden.<ref target="ftn18" n="18"/>
                    </p>
                </div>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>
                    <anchor xml:id="exrx7zlfj85t"/>Forschungsperspektiven
                </head>
                <p style="text-align: left; ">In den Pilotstudien zum audiovisuellen Diskurs zur globalen Finanzkrise (2007–) erwies sich der Annotation Explorer insbesondere im Hinblick auf die Korpusexploration und die Überprüfung von filmübergreifenden Forschungshypothesen als nützliches Werkzeug der Mustererkennung (im Sinne von Bakels et al 2020a, 115f).<ref target="ftn19" n="19"/> Im Kontext der Frage nach der audiovisuellen Gestaltung und Motivik gesellschaftlicher Krisenkommunikation in Nachrichten, Spiel- und Dokumentarfilmen bot das Framework neue Möglichkeiten der Exploration in unterschiedlichen Zusammenhängen. Werden einzelne Aspekte eines audiovisuellen Motivs durch die bereits implementierten (semi-)automatischen Erkenner erfasst, so ermöglichen diese das schnelle Auffinden weiterer Vorkommnisse. So zeigte eine Studie zum Stilmittel des “Brokerface”, dass Bildkompositionen von angespannten Gesichter von Aktienhändlern im Zusammenspiel mit Monitoren einen wiederkehrendes Topos in der Verhandlung der Frage nach den realen Auswirkungen virtueller Marktentwicklungen spielt (vgl. Scherer &amp; Stratil (in Ersch.)). Eine weitere Studie zum Motiv der plötzlich hereinbrechenden BREAKING NEWS als Kipp- und Höhepunkt von Krisendramaturgien veranschaulicht hingegen wie das Zusammenspiel von automatischer Schnitt-, Bildbewegungs- und Lautstärkeerkennung Rhythmusprofile hervorbringt, die Aufschluss über deren affektive Dimension geben (Stratil, in Ersch.). Ein solcher dynamischer ‘Fingerabdruck’ einer Szene, der sich durch die (Darstellungs-)Prinzipien der Prozessualität, Synchronizität und Komparabilität in den Annotationsdaten als Muster zu erkennen gibt, ermöglicht es Aussagen zur Affektorientierung audiovisueller Sequenzen direkt am Material zu plausibilisieren und Charakteristika und Vergleichspunkte herauszuarbeiten. 
                    <hi rend=" background(white)" xml:space="preserve">Die grundlegende Architektur des Annotation Explorers ermöglicht die flexible Kombination von (semi-)automatischen und manuellen Annotationen. </hi>Werden Kernkompositionsprinzipien allerdings nicht durch (semi-)automatische Erkenner tangiert, ist die Analyse auf die zeitaufwändigeren manuellen Annotationen angewiesen. Der Nutzen des Frameworks im Bereich der Korpus-Exploration verschiebt sich dann hin zu mikroanalytischen Detail- und Vergleichsstudien, anhand derer nachvollzogen werden kann, wie kompositorische Prinzipien quer zu einzelnen Gestaltungsebenen als übergreifendes dynamisches Muster in Erscheinung treten. Es konnte so beispielsweise gezeigt werden, wie das Gefühl der Enttäuschung in einem Finanzkrisenfilm nicht als narrative Information zum Ausdruck kommt, sondern als filmische Ausdrucksbewegung (Bakels et al. 2020b).
                </p>
                <p style="text-align: left; ">Die Architektur des Annotation Explorers ermöglicht die Verknüpfung semantischer Daten mit einer geisteswissenschaftlichen Perspektive auf die Expressivität audiovisueller Medien und stellt so Fragen nach der Anwendbarkeit solcher Technologien in qualitativen Ansätzen – nicht als Alternative, sondern als ergänzendes Instrument zur Weitung und Systematisierung des analytischen Blicks.</p>
            </div>
        </body>
        <back><div type="notes"><note place="foot" xml:id="ftn1" n="1"><ref target="http://ada.cinepoetics.org/explorer/">http://ada.cinepoetics.org/explorer/
                            </ref>
                        </note><note place="foot" xml:id="ftn2" n="2"><ref target="https://frametrail.org/">
                                https://frametrail.org/
                            </ref>
                        </note><note place="foot" xml:id="ftn3" n="3"><ref target="https://www.w3.org/DesignIssues/LinkedData.html">
                                    https://www.w3.org/DesignIssues/LinkedData.html
                                </ref>
                            </note><note place="foot" xml:id="ftn4" n="4"><ref target="https://github.com/openlink/virtuoso-opensource/">
                                    https://github.com/openlink/virtuoso-opensource/
                                </ref>
                            </note><note place="foot" xml:id="ftn5" n="5"><ref target="https://github.com/LodLive/LodView">
                                    https://github.com/LodLive/LodView
                                </ref>
                            </note><note place="foot" xml:id="ftn6" n="6"><ref target="https://ada.cinepoetics.org/sparql/">
                                    https://ada.cinepoetics.org/sparql/
                                </ref>
                            </note><note place="foot" xml:id="ftn7" n="7">
                                <hi style="font-size:10pt" xml:space="preserve"> Download der Ontologie unter </hi><ref target="https://github.com/ProjectAdA/ada-ae/tree/main/filmontology">
                                    https://github.com/ProjectAdA/ada-ae/tree/main/filmontology
                                </ref>
                                <hi style="font-size:10pt" xml:space="preserve">, Visualisierung unter </hi><ref target="https://ada.cinepoetics.org/ontoviz/">
                                    https://ada.cinepoetics.org/ontoviz/
                                </ref>
                            </note><note place="foot" xml:id="ftn8" n="8">
                                <hi style="font-size:10pt" xml:space="preserve"> Beispiel einer Ontologie URI: Typ der Kamerabewegung </hi><ref target="http://ada.cinepoetics.org/resource/2021/05/19/AnnotationType/CameraMovementType">
                                    http://ada.cinepoetics.org/resource/2021/05/19/AnnotationType/CameraMovementType
                                </ref>
                            </note><note place="foot" xml:id="ftn9" n="9">
                                <hi style="font-size:10pt" xml:space="preserve"> Beispiel einer Metadaten URI: Occupy Wall Street: </hi><ref target="https://ada.cinepoetics.org/resource/media/39953b6ccea8c49b0a119f1715aab20818e4564cc4b2c2e8567722c9f418f1b9">
                                    https://ada.cinepoetics.org/resource/media/39953b6ccea8c49b0a119f1715aab20818e4564cc4b2c2e8567722c9f418f1b9
                                </ref>
                            </note><note place="foot" xml:id="ftn10" n="10">
                                <hi style="font-size:10pt" xml:space="preserve"> Download der Metadaten unter </hi><ref target="https://github.com/ProjectAdA/ada-ae/tree/main/corpus_metadata">
                                    https://github.com/ProjectAdA/ada-ae/tree/main/corpus_metadata
                                </ref>
                            </note><note place="foot" xml:id="ftn11" n="11"><ref target="https://www.w3.org/TR/annotation-model/">
                                    https://www.w3.org/TR/annotation-model/
                                </ref>
                            </note><note place="foot" xml:id="ftn12" n="12"><ref target="https://www.w3.org/TR/media-frags/">
                                    https://www.w3.org/TR/media-frags/
                                </ref>
                            </note><note place="foot" xml:id="ftn13" n="13">
                                <hi style="font-size:10pt" xml:space="preserve"> Die Annotation ist auch per URI abrufbar:</hi>
                            <ref target="http://ada.cinepoetics.org/resource/media/294704ee3bd55a6888235ae7721120c29522eddd3cc273cc8365fa0eef2ac56d/ed63d084-717f-11e9-99b8-0242ac130003">
                                    <hi style="font-size:10pt">http://ada.cinepoetics.org/resource/media/294704ee3bd55a6888235ae7721120c29522eddd3cc273cc8365fa0eef2ac56d/ed63d084-717f-11e9-99b8-0242ac130003</hi>
                                </ref>
                            </note><note place="foot" xml:id="ftn14" n="14"><ref target="https://jena.apache.org/">
                                    https://jena.apache.org/
                                </ref>
                            </note><note place="foot" xml:id="ftn15" n="15"><ref target="https://javalin.io/">
                                    https://javalin.io/
                                </ref>
                            </note><note place="foot" xml:id="ftn16" n="16"><ref target="https://github.com/jsonld-java/jsonld-java">
                                    https://github.com/jsonld-java/jsonld-java
                                </ref>
                            </note><note place="foot" xml:id="ftn17" n="17"><ref target="https://project1.ada.cinepoetics.org/explorer/?r%5B%5D=t_n_crisis_w_48&amp;ui=kfc&amp;unit=movie&amp;view=movie">
                                    https://project1.ada.cinepoetics.org/explorer/?r[]=t_n_crisis_w_48&amp;ui=kfc&amp;unit=movie&amp;view=movie
                                </ref>
                            </note><note place="foot" xml:id="ftn18" n="18">
                                <hi style="font-size:10pt" xml:space="preserve"> Detaillierte Anleitung zur Benutzung des Annotation Explorer findet sich in </hi><ref target="https://www.ada.cinepoetics.fu-berlin.de/en/ada-toolkit/index.html">
                                    Pfeilschifter et al. 2021
                                </ref>
                                <hi style="font-size:10pt">, S. 151ff.</hi>
                            </note><note place="foot" xml:id="ftn19" n="19">
                            <hi style="font-size:10pt" xml:space="preserve"> Vgl. dazu den Themenschwerpunkt der </hi>
                            <hi rend="italic" style="font-size:10pt">mediaesthetics</hi>
                            <hi style="font-size:10pt" xml:space="preserve">-Ausgabe </hi><ref target="https://www.mediaesthetics.org/index.php/mae/issue/view/12/showToc">
                                <hi rend="underline" style="font-size:10pt">“Digitale Filmanalyse und Bilder der Krise”</hi>
                            </ref>.
                        </note></div>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliographie</head>
                    <bibl style="text-align: left; ">
                        <hi rend="bold" xml:space="preserve">Arnold, Taylor / Tilton, Lauren / Berke, Annie </hi>(2019): “Visual Style in Two Network Era Sitcoms”, in: 
                        <hi rend="italic">Journal of Cultural Analytics</hi> 1(2) 10.22148/16.043.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Aubert, Olivier / Prié, Yannick</hi> (2005): “Advene: active reading through hypervideo”, in: 
                        <hi rend="italic">Proceedings of the sixteenth ACM conference on Hypertext and hypermedia</hi> 235-244.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Bakels, Jan-Hendrik / Grotkopp, Matthias / Scherer, Thomas / Stratil, Jasper</hi> (2020a): “Digitale Empirie? Computergestützte Filmanalyse im Spannungsfeld von Datenmodellen und Gestalttheorie”. In: 
                        <hi rend="italic">montage/AV</hi> 29(1) 99-118.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Bakels, Jan-Hendrik / Grotkopp, Matthias / Scherer, Thomas / Stratil, Jasper</hi> (2020b): “Matching Computational Analysis and Human Experience. Performative Arts and the Digital Humanities”. In: 
                        <hi rend="italic">Digital Humanities Quarterly</hi> 14(4) 
                        <ref target="http://www.digitalhumanities.org/dhq/vol/14/4/000496/000496.html">http://www.digitalhumanities.org/dhq/vol/14/4/000496/000496.html</ref> [letzter Zugriff: 26.11.2021].
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold" xml:space="preserve">Bakels, Jan-Hendrik / Scherer, Thomas / Stratil, Jasper / Agt-Rickauer, Henning </hi>(2020): “AdA Filmontology – a machine-readable Film Analysis Vocabulary for Video Annotation”, in: 
                        <hi rend="italic">Book of Abstracts: DH2020 – carrefours/intersections</hi>
                        <ref target="https://dh2020.adho.org/wp-content/uploads/2020/07/488_AdAFilmontologyamachinereadableFilmAnalysisVocabularyforVideoAnnotation.html">https://dh2020.adho.org/wp-content/uploads/2020/07/488_AdAFilmontologyamachinereadableFilmAnalysisVocabularyforVideoAnnotation.html</ref> [letzter Zugriff: 26.11.2021].
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Flückiger, Barbara / Halter Gaudenz</hi> (2020): “Methods and Advanced Tools for the Analysis of Film Colors in Digital Humanities”. In: 
                        <hi rend="italic">Digital Humanities Quarterly</hi> 14(4) 
                        <ref target="http://www.digitalhumanities.org/dhq/vol/14/4/000500/000500.html">http://www.digitalhumanities.org/dhq/vol/14/4/000500/000500.html</ref> [letzter Zugriff: 26.11.2021].
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Freedman, Richard / Fiala, David / Micah, Walter</hi> (2019): “The Quotable Musical Text in a Digital Age: Modeling Complexity in the Renaissance and Today”. In: <hi rend="italic">Book of Abstracts DH2019</hi>, 
                        <ref target="https://dev.clariah.nl/files/dh2019/boa/0402.html">https://dev.clariah.nl/files/dh2019/boa/0402.html</ref> [letzter Zugriff: 26.11.2021].
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Kappelhoff, Hermann</hi> (2018): 
                        <hi rend="italic">Kognition und Reflexion: Zur Theorie filmischen Denkens</hi>. Berlin/Boston, MA: De Gruyter.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Kappelhoff, Hermann / Bakels, Jan-Hendrik / Berger, Hanno / Brückner, Regina / Böhme, Dorothea / Chung, Hye-Jeung / Dang, Sarah-Mai / Gaertner, David / Greifenstein, Sarah / Gronmaier, Danny / Grotkopp, Matthias / Haupts, Tobias / Illger, Daniel / Lehmann, Hauke / Lück, Michael / Pogodda, Cilli / Roleff, Naomi / Rook, Stefan / Rositzka, Eileen / Scherer, Thomas / Schlochtermeier, Lorna / Schmitt, Christina / Steininger, Anna / Tag, Susanne</hi> (2011-2016): 
                        <hi rend="italic">Empirische Medienästhetik. Datenmatrix Kriegsfilm – eMAEX</hi>.
                        <ref target="https://www.empirische-medienaesthetik.fu-berlin.de/emaex-system/index.html">https://www.empirische-medienaesthetik.fu-berlin.de/emaex-system/index.html</ref> [letzter Zugriff: 26.11.2021].
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Masse, Mark</hi> (2011): 
                        <hi rend="italic">REST API Design Rulebook: Designing Consistent RESTful Web Service Interfaces</hi>. Sebastopol, CA: O'Reilly Media.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Pfeilschifter, Yvonne / Prado, João / Zorko, Rebecca / Buzal, Anton / Scherer, Thomas / Stratil, Jasper / Bakels, Jan-Hendrik</hi> (2021) 
                        <hi rend="italic">Manual: Annotieren mit Advene und der AdA-Filmontologie</hi>. 
                        <ref target="https://www.ada.cinepoetics.fu-berlin.de/ada-toolkit">https://www.ada.cinepoetics.fu-berlin.de/ada-toolkit
                        </ref> [letzter Zugriff: 26.11.2021].
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Scherer, Thomas / Stratil, Jasper</hi> (in Ersch.): “Can’t Read my Broker Face? – Tracing a Motif and Metaphor of Expert Knowledge Through Audiovisual Images of the Financial Crisis”. In 
                        <hi rend="italic">Literature Compass</hi>.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Stratil, Jasper</hi> (in Ersch.): “Geteilte (Medien-)Erinnerung und die Zeiten der Krise. Zum Diskurs audiovisueller Finanzkrisendarstellungen anhand von ‘Breaking News’”, in: 
                        <hi rend="italic">mediaesthetics</hi> (4).
                        <anchor xml:id="jkpqn3862o3o"/>
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>
